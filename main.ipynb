{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Heart Disease Classification\n",
        "\n",
        "# Heart Disease Classification with a Feed-Forward Neural Network\n",
        "# This notebook demonstrates the use of a feed-forward neural network for heart disease classification using the heart disease dataset.\n",
        "\n",
        "# Step 1: Import Necessary Libraries and Functions\n",
        "# We start by importing our custom module \"neural_network_module.py\" and other necessary libraries.\n",
        "\n",
        "import pandas as pd\n",
        "from neural_network_module import NeuralNetwork, BuildDataset, train_one_by_one, evaluate\n",
        "import torch\n",
        "\n",
        "# Step 2: Load the Dataset\n",
        "# We'll load the heart disease dataset and display the first few rows to understand its structure.\n",
        "\n",
        "# Load dataset\n",
        "data = pd.read_csv('/content/heart.csv')\n",
        "data.head()\n",
        "\n",
        "# Step 3: Prepare the Dataset\n",
        "# We use the \"BuildDataset\" function from our module to split the dataset into training and testing sets and scale the features.\n",
        "\n",
        "# Prepare dataset\n",
        "X_train, X_test, y_train, y_test = BuildDataset(data)\n",
        "\n",
        "# Display the shapes of the training and testing sets to confirm successful preparation\n",
        "print(f\"Training set shape: {X_train.shape}\")\n",
        "print(f\"Test set shape: {X_test.shape}\")\n",
        "\n",
        "# Step 4: Initialize the Neural Network Model\n",
        "# We define the neural network model using the `NeuralNetwork` class from our module, specifying the input size based on our dataset.\n",
        "\n",
        "# Define model\n",
        "input_size = X_train.shape[1]  # Number of features\n",
        "model = NeuralNetwork(input_size=input_size)\n",
        "\n",
        "# Step 5: Train the Model One Epoch at a Time\n",
        "# We train the model using the \"train_one_by_one\" function, which trains the model for a specified number of epochs and displays the loss after each epoch.\n",
        "\n",
        "# Train the model (this may take a few minutes depending on the number of epochs)\n",
        "train_one_by_one(model, X_train, y_train, epochs=100, learning_rate=0.001)\n",
        "\n",
        "# Step 6: Evaluate the Model\n",
        "# After training, we evaluate the model on the test set using the `evaluate` function, which calculates and prints the test accuracy.\n",
        "\n",
        "# now we evaluate the model\n",
        "accuracy = evaluate(model, X_test, y_test)\n",
        "print(f\"Final Test Accuracy: {accuracy * 100:.2f}%\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uLEIYO56K8-j",
        "outputId": "6effa246-0b58-46fd-c553-ac0fe0a7796a"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training set shape: torch.Size([820, 13])\n",
            "Test set shape: torch.Size([205, 13])\n",
            "Epoch [1/100], Loss: 0.7058\n",
            "Epoch [2/100], Loss: 0.7005\n",
            "Epoch [3/100], Loss: 0.6953\n",
            "Epoch [4/100], Loss: 0.6903\n",
            "Epoch [5/100], Loss: 0.6853\n",
            "Epoch [6/100], Loss: 0.6804\n",
            "Epoch [7/100], Loss: 0.6755\n",
            "Epoch [8/100], Loss: 0.6706\n",
            "Epoch [9/100], Loss: 0.6656\n",
            "Epoch [10/100], Loss: 0.6606\n",
            "Epoch [11/100], Loss: 0.6555\n",
            "Epoch [12/100], Loss: 0.6503\n",
            "Epoch [13/100], Loss: 0.6450\n",
            "Epoch [14/100], Loss: 0.6395\n",
            "Epoch [15/100], Loss: 0.6339\n",
            "Epoch [16/100], Loss: 0.6281\n",
            "Epoch [17/100], Loss: 0.6221\n",
            "Epoch [18/100], Loss: 0.6158\n",
            "Epoch [19/100], Loss: 0.6094\n",
            "Epoch [20/100], Loss: 0.6028\n",
            "Epoch [21/100], Loss: 0.5961\n",
            "Epoch [22/100], Loss: 0.5891\n",
            "Epoch [23/100], Loss: 0.5820\n",
            "Epoch [24/100], Loss: 0.5747\n",
            "Epoch [25/100], Loss: 0.5673\n",
            "Epoch [26/100], Loss: 0.5597\n",
            "Epoch [27/100], Loss: 0.5519\n",
            "Epoch [28/100], Loss: 0.5440\n",
            "Epoch [29/100], Loss: 0.5359\n",
            "Epoch [30/100], Loss: 0.5278\n",
            "Epoch [31/100], Loss: 0.5195\n",
            "Epoch [32/100], Loss: 0.5112\n",
            "Epoch [33/100], Loss: 0.5028\n",
            "Epoch [34/100], Loss: 0.4945\n",
            "Epoch [35/100], Loss: 0.4861\n",
            "Epoch [36/100], Loss: 0.4778\n",
            "Epoch [37/100], Loss: 0.4695\n",
            "Epoch [38/100], Loss: 0.4614\n",
            "Epoch [39/100], Loss: 0.4533\n",
            "Epoch [40/100], Loss: 0.4455\n",
            "Epoch [41/100], Loss: 0.4378\n",
            "Epoch [42/100], Loss: 0.4302\n",
            "Epoch [43/100], Loss: 0.4229\n",
            "Epoch [44/100], Loss: 0.4158\n",
            "Epoch [45/100], Loss: 0.4090\n",
            "Epoch [46/100], Loss: 0.4024\n",
            "Epoch [47/100], Loss: 0.3961\n",
            "Epoch [48/100], Loss: 0.3901\n",
            "Epoch [49/100], Loss: 0.3843\n",
            "Epoch [50/100], Loss: 0.3788\n",
            "Epoch [51/100], Loss: 0.3735\n",
            "Epoch [52/100], Loss: 0.3685\n",
            "Epoch [53/100], Loss: 0.3637\n",
            "Epoch [54/100], Loss: 0.3591\n",
            "Epoch [55/100], Loss: 0.3548\n",
            "Epoch [56/100], Loss: 0.3507\n",
            "Epoch [57/100], Loss: 0.3468\n",
            "Epoch [58/100], Loss: 0.3432\n",
            "Epoch [59/100], Loss: 0.3397\n",
            "Epoch [60/100], Loss: 0.3363\n",
            "Epoch [61/100], Loss: 0.3331\n",
            "Epoch [62/100], Loss: 0.3301\n",
            "Epoch [63/100], Loss: 0.3273\n",
            "Epoch [64/100], Loss: 0.3245\n",
            "Epoch [65/100], Loss: 0.3220\n",
            "Epoch [66/100], Loss: 0.3195\n",
            "Epoch [67/100], Loss: 0.3172\n",
            "Epoch [68/100], Loss: 0.3149\n",
            "Epoch [69/100], Loss: 0.3127\n",
            "Epoch [70/100], Loss: 0.3106\n",
            "Epoch [71/100], Loss: 0.3086\n",
            "Epoch [72/100], Loss: 0.3067\n",
            "Epoch [73/100], Loss: 0.3048\n",
            "Epoch [74/100], Loss: 0.3030\n",
            "Epoch [75/100], Loss: 0.3013\n",
            "Epoch [76/100], Loss: 0.2996\n",
            "Epoch [77/100], Loss: 0.2979\n",
            "Epoch [78/100], Loss: 0.2963\n",
            "Epoch [79/100], Loss: 0.2948\n",
            "Epoch [80/100], Loss: 0.2933\n",
            "Epoch [81/100], Loss: 0.2918\n",
            "Epoch [82/100], Loss: 0.2903\n",
            "Epoch [83/100], Loss: 0.2889\n",
            "Epoch [84/100], Loss: 0.2874\n",
            "Epoch [85/100], Loss: 0.2860\n",
            "Epoch [86/100], Loss: 0.2846\n",
            "Epoch [87/100], Loss: 0.2833\n",
            "Epoch [88/100], Loss: 0.2820\n",
            "Epoch [89/100], Loss: 0.2807\n",
            "Epoch [90/100], Loss: 0.2794\n",
            "Epoch [91/100], Loss: 0.2781\n",
            "Epoch [92/100], Loss: 0.2768\n",
            "Epoch [93/100], Loss: 0.2756\n",
            "Epoch [94/100], Loss: 0.2744\n",
            "Epoch [95/100], Loss: 0.2731\n",
            "Epoch [96/100], Loss: 0.2719\n",
            "Epoch [97/100], Loss: 0.2707\n",
            "Epoch [98/100], Loss: 0.2695\n",
            "Epoch [99/100], Loss: 0.2683\n",
            "Epoch [100/100], Loss: 0.2672\n",
            "Test Accuracy: 84.39%\n",
            "Final Test Accuracy: 84.39%\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}